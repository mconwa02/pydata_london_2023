{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyData London June 2023 pydata.org Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. Pydata Demos and Talks Sktime - python toolbox for time series An Introduction to Polars Bring best practices to your messy data science team! Delta Lake 101: How many water metaphors does it take to describe data? Large Language Models: From Prototype to Production The Opinionated Python Stack Executives at PyData Fast API facts we wish we'd known beforehand 09_pandas_polar.md 10_new_dev_pandas_and_dask.md 11_correlation_to_causation.md Built using MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Home"},{"location":"#pydata-london-june-2023","text":"pydata.org","title":"PyData London June 2023"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage.","title":"Project layout"},{"location":"#pydata-demos-and-talks","text":"Sktime - python toolbox for time series An Introduction to Polars Bring best practices to your messy data science team! Delta Lake 101: How many water metaphors does it take to describe data? Large Language Models: From Prototype to Production The Opinionated Python Stack Executives at PyData Fast API facts we wish we'd known beforehand 09_pandas_polar.md 10_new_dev_pandas_and_dask.md 11_correlation_to_causation.md","title":"Pydata Demos and Talks"},{"location":"#built-using-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Built using MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"01_sktime/","text":"01 sktime - python toolbox for time series: how to implement your own estimator 06-17, 09:00\u201310:30 (Europe/London), Tower Suite 1 https://github.com/sktime/sktime-tutorial-pydata-london-2023/tree/main sktime is a widely used scikit-learn compatible library for learning with time series. sktime is easily extensible by anyone, and interoperable with the pydata/numfocus stack. This tutorial explains how to write your own sktime estimator, e.g., forecaster, classifier, transformer, by using sktime\u2019s extension templates and testing framework. A custom estimator can live in any local code base, and will be compatible with sktime pipelines, or scikit-learn. A continuation of the sktime introductory tutorial at pydata [link] Writing sktime compatible estimators is meant to be easy. This tutorial will explain: \u2022 sktime base class and estimator architecture \u2022 basic software design patterns used in extension \u2022 how to use the extension templates \u2022 how to validate your custom estimator \u2022 testing in third party extensions and packages Users can write sktime compatible estimators without a full developer setup, or any need to contribute the estimator to the sktime codebase. The custom estimator can be used with any tuning, pipeline, composition, or reduction functionality in sktime, and will be compatible with scikit-learn, too. This philosophy enables interoperability with third projects, proprietary code bases, or custom extension packages to sktime. How this works technically: sktime ensures that all estimators of a certain type, e.g., forecasters, adhere to the same interface contracts, by using the base class and strategy patterns. Separate to this user sided contract is the extension contract, which \"extenders\", users implementing their own estimators, have to satisfy. This is based on the template pattern which keeps boilerplate from the extension contract, and clearly defined \"fill in your code\" instructions in sktime\u00b4s extension templates. The extension templates are python files with gaps that the extender is meant to fill in with the logic of a new estimator, with clear instructions in comments, and without any boilerplate. Finally, the sktime test suite provides few-line-validation for any custom estimator. A full developer setup is typically not required to implement a custom estimator compatible with sktime.","title":"sktime talk"},{"location":"01_sktime/#01-sktime-python-toolbox-for-time-series","text":"how to implement your own estimator 06-17, 09:00\u201310:30 (Europe/London), Tower Suite 1 https://github.com/sktime/sktime-tutorial-pydata-london-2023/tree/main sktime is a widely used scikit-learn compatible library for learning with time series. sktime is easily extensible by anyone, and interoperable with the pydata/numfocus stack. This tutorial explains how to write your own sktime estimator, e.g., forecaster, classifier, transformer, by using sktime\u2019s extension templates and testing framework. A custom estimator can live in any local code base, and will be compatible with sktime pipelines, or scikit-learn. A continuation of the sktime introductory tutorial at pydata [link] Writing sktime compatible estimators is meant to be easy. This tutorial will explain: \u2022 sktime base class and estimator architecture \u2022 basic software design patterns used in extension \u2022 how to use the extension templates \u2022 how to validate your custom estimator \u2022 testing in third party extensions and packages Users can write sktime compatible estimators without a full developer setup, or any need to contribute the estimator to the sktime codebase. The custom estimator can be used with any tuning, pipeline, composition, or reduction functionality in sktime, and will be compatible with scikit-learn, too. This philosophy enables interoperability with third projects, proprietary code bases, or custom extension packages to sktime. How this works technically: sktime ensures that all estimators of a certain type, e.g., forecasters, adhere to the same interface contracts, by using the base class and strategy patterns. Separate to this user sided contract is the extension contract, which \"extenders\", users implementing their own estimators, have to satisfy. This is based on the template pattern which keeps boilerplate from the extension contract, and clearly defined \"fill in your code\" instructions in sktime\u00b4s extension templates. The extension templates are python files with gaps that the extender is meant to fill in with the logic of a new estimator, with clear instructions in comments, and without any boilerplate. Finally, the sktime test suite provides few-line-validation for any custom estimator. A full developer setup is typically not required to implement a custom estimator compatible with sktime.","title":"01 sktime - python toolbox for time series:"},{"location":"02_polars/","text":"02 An Introduction to Polars 06-02, 11:00\u201312:30 (Europe/London), Minories https://tinyurl.com/pydatapolars Uses Rust terms Polars is a next generation data-frame library which aims to be fast, efficient, composable and lazy! This introductory tutorial will take you through the basics of getting started with polars in Python. We will demonstrate the out the box multi-core efficiencies, by composing advanced filters and joins, before comparing with the traditional pandas workflows. As a finale we will look at some lazy processing when applying polars to large scale data-sets. Background The tutorial targets intermediate data-scientists who use pandas as a part of their existing data science tool-kit. The central premise of the tutorial is that polars is faster and more composable resulting in a cleaner and more productive work flows. The ultimate aim of this tutorial is to \"convert\" pandas users to polars :) ! Introduction Installation - basic installation using pip Data type basics - column types and coalescing Interop with pandas/numpy - how this relates to the traditional numpy dtypes File reading basics - the standard operations to read data into a dataframe from a host of different formats Standard Workflow Accessing columns Filtering - filtering is composed quite nicely in polars, so we will go through a few examples Grouping - grouping is again nicely multicore Joining Row based operations Advanced Workflow A note about multicore - polars is Rust under the hood and the correctness allows for a clean multicore processing capacity. We will spend five minutes demonstrating this on a large data-set. Case study, lazy geospatial processing: The final part of the tutorial will be a case study example of efficient geospatial lazy processing. In this we will go through the efficiency gains of using the lazy interface to filter a large collection of geospatial data in a multicore way, to find points within defined polygonal shapes. We will show that large amounts of data can be processed efficiently even on relative small setup, and complex filters can be applied to disk backed data.","title":"polars"},{"location":"02_polars/#02-an-introduction-to-polars","text":"06-02, 11:00\u201312:30 (Europe/London), Minories https://tinyurl.com/pydatapolars Uses Rust terms Polars is a next generation data-frame library which aims to be fast, efficient, composable and lazy! This introductory tutorial will take you through the basics of getting started with polars in Python. We will demonstrate the out the box multi-core efficiencies, by composing advanced filters and joins, before comparing with the traditional pandas workflows. As a finale we will look at some lazy processing when applying polars to large scale data-sets.","title":"02 An Introduction to Polars"},{"location":"02_polars/#background","text":"The tutorial targets intermediate data-scientists who use pandas as a part of their existing data science tool-kit. The central premise of the tutorial is that polars is faster and more composable resulting in a cleaner and more productive work flows. The ultimate aim of this tutorial is to \"convert\" pandas users to polars :) !","title":"Background"},{"location":"02_polars/#introduction","text":"Installation - basic installation using pip Data type basics - column types and coalescing Interop with pandas/numpy - how this relates to the traditional numpy dtypes File reading basics - the standard operations to read data into a dataframe from a host of different formats","title":"Introduction"},{"location":"02_polars/#standard-workflow","text":"Accessing columns Filtering - filtering is composed quite nicely in polars, so we will go through a few examples Grouping - grouping is again nicely multicore","title":"Standard Workflow"},{"location":"02_polars/#joining","text":"Row based operations Advanced Workflow A note about multicore - polars is Rust under the hood and the correctness allows for a clean multicore processing capacity. We will spend five minutes demonstrating this on a large data-set. Case study, lazy geospatial processing: The final part of the tutorial will be a case study example of efficient geospatial lazy processing. In this we will go through the efficiency gains of using the lazy interface to filter a large collection of geospatial data in a multicore way, to find points within defined polygonal shapes. We will show that large amounts of data can be processed efficiently even on relative small setup, and complex filters can be applied to disk backed data.","title":"Joining"},{"location":"03_best_practises/","text":"03 Bring best practices to your messy data science team! 06-02, 13:30\u201315:00 (Europe/London), Minories This session's header image If part of your job is to constantly poke your fellow data scientist to isolate projects environments, updating requirements, cleaning code, writing consistent docstrings, etc., then you should definitely join us for this very hands-on tutorial with reproducibility, compliance, and consistency in mind We will be covering: - What is pipx and why using it - Managing virtual environments and dependencies using pyproject.toml, venv, and pip-tools - Running pre-commit hooks (black, ruff, isort, pydocstyle, sqlfluff, and a few more!) - Automating commands using Make with a Makefile - Bonus: VSCode integration Code and slides: https://github.com/DrGabrielHarris/pydata-london-2023 https://www.gnu.org/home.en.html Website: https://12ft.io/ Not using in Poetry instead using pip-tools https://pip-tools.readthedocs.io/en/latest/ https://www.pydocstyle.org/en/stable/ https://github.com/charliermarsh/ruff Questions for talk https://www.canva.com/presentation/join Package squatting Typo-squatting occurs when a malicious package is uploaded with a name similar to a common package ex pip install panda instead of pip install pandas","title":"python best practises"},{"location":"03_best_practises/#03-bring-best-practices-to-your-messy-data-science-team","text":"06-02, 13:30\u201315:00 (Europe/London), Minories This session's header image If part of your job is to constantly poke your fellow data scientist to isolate projects environments, updating requirements, cleaning code, writing consistent docstrings, etc., then you should definitely join us for this very hands-on tutorial with reproducibility, compliance, and consistency in mind We will be covering: - What is pipx and why using it - Managing virtual environments and dependencies using pyproject.toml, venv, and pip-tools - Running pre-commit hooks (black, ruff, isort, pydocstyle, sqlfluff, and a few more!) - Automating commands using Make with a Makefile - Bonus: VSCode integration Code and slides: https://github.com/DrGabrielHarris/pydata-london-2023 https://www.gnu.org/home.en.html Website: https://12ft.io/ Not using in Poetry instead using pip-tools https://pip-tools.readthedocs.io/en/latest/ https://www.pydocstyle.org/en/stable/ https://github.com/charliermarsh/ruff Questions for talk https://www.canva.com/presentation/join","title":"03 Bring best practices to your messy data science team!"},{"location":"03_best_practises/#package-squatting","text":"Typo-squatting occurs when a malicious package is uploaded with a name similar to a common package ex pip install panda instead of pip install pandas","title":"Package squatting"},{"location":"04_delta_lake/","text":"Delta Lake 101: How many water metaphors does it take to describe data? 06-02, 15:30\u201317:00 (Europe/London), Warwick Delta Lake is an open-source storage framework that enables the creation of a Lakehouse architecture using a variety of compute engines such as Spark, PrestoDB, Flink, Trino, and Hive from Python. Its high data reliability and optimized query performance make it an ideal solution for supporting big data use cases, including batch and streaming data ingestion, fast interactive queries, and machine learning. https://github.com/delta-io/delta Apple was using parquet files and it didn't scale so delta got created In this tutorial, you will learn about the current requirements in modern data engineering and the challenges faced by data engineers in ensuring data reliability and performance. We will delve into how Delta Lake can help overcome these obstacles, through presentations, hands-on code examples and notebooks. By the end of the tutorial, you will have a comprehensive understanding of how Delta Lake can be applied to your data architecture and the benefits it can bring. Additionally, you will gain insight into how the wider open-source community is utilizing Delta Lake as an open standard to develop the next generation of data engineering and data science tools in Python. https://delta.io/ 1.7+ Exabytes processed a day 7k companies Delta Rust API delta-rs: This library provides low level access to Delta tables in Rust, which can be used with data processing frameworks like datafusion, ballista, polars, vega, etc. It also provides bindings to other higher level languages such as Python or Ruby. https://hub.docker.com/r/deltaio/delta-sharing-server https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.to_pandas.html","title":"Spark an Delta Lakes"},{"location":"04_delta_lake/#delta-lake-101-how-many-water-metaphors-does-it-take-to-describe-data","text":"06-02, 15:30\u201317:00 (Europe/London), Warwick Delta Lake is an open-source storage framework that enables the creation of a Lakehouse architecture using a variety of compute engines such as Spark, PrestoDB, Flink, Trino, and Hive from Python. Its high data reliability and optimized query performance make it an ideal solution for supporting big data use cases, including batch and streaming data ingestion, fast interactive queries, and machine learning. https://github.com/delta-io/delta Apple was using parquet files and it didn't scale so delta got created In this tutorial, you will learn about the current requirements in modern data engineering and the challenges faced by data engineers in ensuring data reliability and performance. We will delve into how Delta Lake can help overcome these obstacles, through presentations, hands-on code examples and notebooks. By the end of the tutorial, you will have a comprehensive understanding of how Delta Lake can be applied to your data architecture and the benefits it can bring. Additionally, you will gain insight into how the wider open-source community is utilizing Delta Lake as an open standard to develop the next generation of data engineering and data science tools in Python. https://delta.io/ 1.7+ Exabytes processed a day 7k companies","title":"Delta Lake 101: How many water metaphors does it take to describe data?"},{"location":"04_delta_lake/#delta-rust-api","text":"delta-rs: This library provides low level access to Delta tables in Rust, which can be used with data processing frameworks like datafusion, ballista, polars, vega, etc. It also provides bindings to other higher level languages such as Python or Ruby. https://hub.docker.com/r/deltaio/delta-sharing-server https://spark.apache.org/docs/3.2.0/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.to_pandas.html","title":"Delta Rust API"},{"location":"05_large_language_models/","text":"Keynote: Large Language Models: From Prototype to Production 06-03, 09:00\u201309:45 (Europe/London), Warwick Keynote with Ines Montani Ines Montani Ines Montani is a developer specializing in tools for AI and NLP technology. She\u2019s the co-founder and CEO of Explosion and a core developer of spaCy, a popular open-source library for Natural Language Processing in Python, and Prodigy, a modern annotation tool for creating training data for machine learning models.","title":"LLMs"},{"location":"05_large_language_models/#keynote-large-language-models-from-prototype-to-production","text":"06-03, 09:00\u201309:45 (Europe/London), Warwick Keynote with Ines Montani Ines Montani Ines Montani is a developer specializing in tools for AI and NLP technology. She\u2019s the co-founder and CEO of Explosion and a core developer of spaCy, a popular open-source library for Natural Language Processing in Python, and Prodigy, a modern annotation tool for creating training data for machine learning models.","title":"Keynote: Large Language Models: From Prototype to Production"},{"location":"06_ml_platforms/","text":"The Opinionated Python Stack I chose for my Company\u2019s ML Projects and how I bundled it in a Project Generator 06-03, 10:15\u201310:55 (Europe/London), Salisbury Have you ever struggled with choosing the right tools for your Machine Learning projects? As a Lead Data Scientist in a consulting firm, I faced this challenge repeatedly and finally converged to a small set of technologies which allow to build reliable and scalable projects with a great DX (Developer Experience). In this talk, I will share the key components of my ML stack, including DVC, Streamlit, FastAPI, Terraform and other powerful tools to streamline the development and experimentation processes. Through a live demo, I will finally show you the Project Generator I\u2019ve built to encourage adoption of these technologies and to help Data Scientists focus on the ML itself rather than the \"plumbing\" around it. Attendees should have a basic understanding of Python and Machine Learning concepts. The world of Data Science is constantly evolving, with new tools and techniques frequently emerging. As a result, building a reliable and efficient Machine Learning stack can be a daunting task. To succeed, companies must navigate a complex landscape of technologies and identify the ones that best meet their unique needs and challenges. As a Lead Data Scientist for a consulting firm, I have gained experience with various projects and have developed my own idea of such a Machine Learning stack, putting together the technologies I've found to be sufficiently mature and flexible. In this talk, I'll walk you through this stack, which includes: - Standard Python developing tools (Poetry, linters, continuous integration, pre-commit, etc.) to ensure a high-quality code base - DVC for data versioning - Streamlit to explore data and analyse experimentation results - FastAPI to serve the model - Terraform for infrastructure as code I'll explain how each component fits into the larger picture and discuss the benefits and trade-offs of using these tools together. To facilitate the adoption of the stack at my company, I've built a Project Generator that allows to quickly and easily scaffold out a new ML project with all of the necessary tools and architecture already in place. This generator helps data scientists focus on the ML itself rather than the \"plumbing\" around it. After a live demo of the tool, I will briefly explain how I built the generator itself. This talk is intended for Data Scientists and ML engineers who want to streamline their work and learn about efficient tools for building reliable and scalable ML projects. Attendees should have a basic understanding of Python and Machine Learning concepts. Talk Notes Quality Checks - black (code formatting) - pytest (unit tests) - mypy (type checking) - Ruff (linting) - pip-tools Code version with Git Data Version Control with DVC Git-like CLI Data Pipeline Write DAGs in YSML syntax Efficient Caching Streamlit wed apps few lines of code https://github.com/sicara/sicarator","title":"ML Platforms"},{"location":"06_ml_platforms/#the-opinionated-python-stack","text":"I chose for my Company\u2019s ML Projects and how I bundled it in a Project Generator 06-03, 10:15\u201310:55 (Europe/London), Salisbury Have you ever struggled with choosing the right tools for your Machine Learning projects? As a Lead Data Scientist in a consulting firm, I faced this challenge repeatedly and finally converged to a small set of technologies which allow to build reliable and scalable projects with a great DX (Developer Experience). In this talk, I will share the key components of my ML stack, including DVC, Streamlit, FastAPI, Terraform and other powerful tools to streamline the development and experimentation processes. Through a live demo, I will finally show you the Project Generator I\u2019ve built to encourage adoption of these technologies and to help Data Scientists focus on the ML itself rather than the \"plumbing\" around it. Attendees should have a basic understanding of Python and Machine Learning concepts. The world of Data Science is constantly evolving, with new tools and techniques frequently emerging. As a result, building a reliable and efficient Machine Learning stack can be a daunting task. To succeed, companies must navigate a complex landscape of technologies and identify the ones that best meet their unique needs and challenges. As a Lead Data Scientist for a consulting firm, I have gained experience with various projects and have developed my own idea of such a Machine Learning stack, putting together the technologies I've found to be sufficiently mature and flexible. In this talk, I'll walk you through this stack, which includes: - Standard Python developing tools (Poetry, linters, continuous integration, pre-commit, etc.) to ensure a high-quality code base - DVC for data versioning - Streamlit to explore data and analyse experimentation results - FastAPI to serve the model - Terraform for infrastructure as code I'll explain how each component fits into the larger picture and discuss the benefits and trade-offs of using these tools together. To facilitate the adoption of the stack at my company, I've built a Project Generator that allows to quickly and easily scaffold out a new ML project with all of the necessary tools and architecture already in place. This generator helps data scientists focus on the ML itself rather than the \"plumbing\" around it. After a live demo of the tool, I will briefly explain how I built the generator itself. This talk is intended for Data Scientists and ML engineers who want to streamline their work and learn about efficient tools for building reliable and scalable ML projects. Attendees should have a basic understanding of Python and Machine Learning concepts.","title":"The Opinionated Python Stack"},{"location":"06_ml_platforms/#talk-notes","text":"Quality Checks - black (code formatting) - pytest (unit tests) - mypy (type checking) - Ruff (linting) - pip-tools Code version with Git Data Version Control with DVC Git-like CLI Data Pipeline Write DAGs in YSML syntax Efficient Caching Streamlit wed apps few lines of code https://github.com/sicara/sicarator","title":"Talk Notes"},{"location":"07_pydata_for_executives/","text":"Executives at PyData 06-03, 11:00\u201311:40 (Europe/London), Beaumont Executives at PyData is a facilitated discussion session for executives and leaders to discuss challenges around designing and delivering successful data projects, organizational communication, product management and design, hiring, and team growth. We'll announce the agenda at the start of the session, you can ask questions or raise issues to get feedback from other leaders in the room, NumFOCUS board members and Ian and James. Organized by Ian Ozsvald (London) and James Powell (New York) Executives at PyData is a facilitated discussion session for executives and leaders to discuss challenges around designing and delivering successful data projects, organizational communication, product management and design, hiring, and team growth. We'll announce the agenda at the start of the session, you can ask questions or raise issues to get feedback from other leaders in the room, NumFOCUS board members and Ian and James. Prior Knowledge Expected \u2013 Previous knowledge expected Ian Ozsvald Ian is a Chief Data Scientist, has helped co-organise the annual PyDataLondon conference raising $100k+ annually for the open source movement along with the associated 11,000+ member monthly meetup. Using data science he's helped clients find $2M in recoverable fraud, created the core IP which opened funding rounds for automated recruitment start-ups and diagnosed how major media companies can better supply recommendations to viewers. He gives conference talks internationally often as keynote speaker and is the author of the bestselling O'Reilly book High Performance Python (2nd edition). He has over 25 years of experience as a senior data science leader, trainer and team coach. For fun he's walked by his high-energy Springer Spaniel, surfs the Cornish coast and drinks fine coffee. Past talks and articles can be found at:","title":"PyData Executives"},{"location":"07_pydata_for_executives/#executives-at-pydata","text":"06-03, 11:00\u201311:40 (Europe/London), Beaumont Executives at PyData is a facilitated discussion session for executives and leaders to discuss challenges around designing and delivering successful data projects, organizational communication, product management and design, hiring, and team growth. We'll announce the agenda at the start of the session, you can ask questions or raise issues to get feedback from other leaders in the room, NumFOCUS board members and Ian and James. Organized by Ian Ozsvald (London) and James Powell (New York) Executives at PyData is a facilitated discussion session for executives and leaders to discuss challenges around designing and delivering successful data projects, organizational communication, product management and design, hiring, and team growth. We'll announce the agenda at the start of the session, you can ask questions or raise issues to get feedback from other leaders in the room, NumFOCUS board members and Ian and James. Prior Knowledge Expected \u2013 Previous knowledge expected Ian Ozsvald Ian is a Chief Data Scientist, has helped co-organise the annual PyDataLondon conference raising $100k+ annually for the open source movement along with the associated 11,000+ member monthly meetup. Using data science he's helped clients find $2M in recoverable fraud, created the core IP which opened funding rounds for automated recruitment start-ups and diagnosed how major media companies can better supply recommendations to viewers. He gives conference talks internationally often as keynote speaker and is the author of the bestselling O'Reilly book High Performance Python (2nd edition). He has over 25 years of experience as a senior data science leader, trainer and team coach. For fun he's walked by his high-energy Springer Spaniel, surfs the Cornish coast and drinks fine coffee. Past talks and articles can be found at:","title":"Executives at PyData"},{"location":"08_fast_ape/","text":"\u2728 fastAPI facts we wish we'd known beforehand. Spoiler: It's not about getting started. 06-03, 11:45\u201312:25 (Europe/London), Salisbury An exchange of views on fastAPI in practice. FastAPI has become an integral part of the PyData ecosytem. FastAPI is great, it helps many engineers create REST APIs based on the OpenAPI standard and run them asynchronously. It has a thriving community and educational documentation. FastAPI does a great job of getting people started with APIs quickly. This talk will point out some obstacles and dark spots that I wish we had known about before. In this talk we want to highlight solutions based on experience building a data hub in asset management. An exchange of views on fastAPI in practice. FastAPI is great, it helps many developers create REST APIs based on the OpenAPI standard and run them asynchronously. It has a thriving community and educational documentation. FastAPI does a great job of getting people started with APIs quickly. This talk will point out some obstacles and dark spots that I wish we had known about before. In this talk we want to highlight solutions. This talk will include the following: pydantic fastAPI is built on the shoulders of giants I: pydantic FastAPI makes extensive use of [pydantic. pydantic]!(https://docs.pydantic.dev/) parses data, can validate (and transform) data, and has built-in interfaces to export OpenAPI definitions among many other features. starlette fastAPI is built on the shoulders of giants I: [starlette]!(https://www.starlette.io/) Routes and middleware are managed by starlette. In this section we will explore how to create custom middleware and what we learned along the way. fastAPI has tutorials, but is this documentation? The fastAPI page provides a good introduction. The more we worked with fastAPI, the harder it was to find accurate documentation. Looking at the source code, we really missed DocStrings! Introspection to the rescue - will probably include a rant about missing DocStrings! pydantic DRY (\"Don't repeat yourself\") with pydantic For our use case, we decided to use strict models to validate our data structures, as we work in a highly regulated industry where no mistakes are allowed to happen. Setting up the REST API was much easier than developing consistent models that generalise well. We follow the \"single source of truth\" paradigm, entering redundant definitions is an absolute no-go. In this section we show how to create highly reusable pydantic model pools with inheritance for use in fastAPI. For testing, we also created models from metadata! \"The road not taken\": pydantic Depends()! API routes often consist of a request model and a response model. But what about cases where the models alone don't work and a model and e.g. query parameters need to be mixed? Apart from flake8 complaining about having callables in the signature, this can be quite a difficult use case. Strategies for resolving model/parameter conflicts. Bonus - if time: Integrating fastAPI with Sphinx. Demonstrate how to integrate OpenAPI with your Sphinx documentation. Based on experience building a data hub in asset management, the talk will show how fastAPI is built and how well introspection can help you understand what is going on under the hood and which library is actually doing the heavy lifting where. Why Fast API High performance web framework instead of flask custom python package interface to FASTAPI current version 259 classes 6022 lines of code more docstrings","title":"Fast API"},{"location":"08_fast_ape/#fastapi-facts-we-wish-wed-known-beforehand","text":"Spoiler: It's not about getting started. 06-03, 11:45\u201312:25 (Europe/London), Salisbury An exchange of views on fastAPI in practice. FastAPI has become an integral part of the PyData ecosytem. FastAPI is great, it helps many engineers create REST APIs based on the OpenAPI standard and run them asynchronously. It has a thriving community and educational documentation. FastAPI does a great job of getting people started with APIs quickly. This talk will point out some obstacles and dark spots that I wish we had known about before. In this talk we want to highlight solutions based on experience building a data hub in asset management. An exchange of views on fastAPI in practice. FastAPI is great, it helps many developers create REST APIs based on the OpenAPI standard and run them asynchronously. It has a thriving community and educational documentation. FastAPI does a great job of getting people started with APIs quickly. This talk will point out some obstacles and dark spots that I wish we had known about before. In this talk we want to highlight solutions. This talk will include the following:","title":"\u2728 fastAPI facts we wish we'd known beforehand."},{"location":"08_fast_ape/#pydantic","text":"fastAPI is built on the shoulders of giants I: pydantic FastAPI makes extensive use of [pydantic. pydantic]!(https://docs.pydantic.dev/) parses data, can validate (and transform) data, and has built-in interfaces to export OpenAPI definitions among many other features.","title":"pydantic"},{"location":"08_fast_ape/#starlette","text":"fastAPI is built on the shoulders of giants I: [starlette]!(https://www.starlette.io/) Routes and middleware are managed by starlette. In this section we will explore how to create custom middleware and what we learned along the way. fastAPI has tutorials, but is this documentation? The fastAPI page provides a good introduction. The more we worked with fastAPI, the harder it was to find accurate documentation. Looking at the source code, we really missed DocStrings! Introspection to the rescue - will probably include a rant about missing DocStrings!","title":"starlette"},{"location":"08_fast_ape/#pydantic_1","text":"DRY (\"Don't repeat yourself\") with pydantic For our use case, we decided to use strict models to validate our data structures, as we work in a highly regulated industry where no mistakes are allowed to happen. Setting up the REST API was much easier than developing consistent models that generalise well. We follow the \"single source of truth\" paradigm, entering redundant definitions is an absolute no-go. In this section we show how to create highly reusable pydantic model pools with inheritance for use in fastAPI. For testing, we also created models from metadata! \"The road not taken\": pydantic Depends()! API routes often consist of a request model and a response model. But what about cases where the models alone don't work and a model and e.g. query parameters need to be mixed? Apart from flake8 complaining about having callables in the signature, this can be quite a difficult use case. Strategies for resolving model/parameter conflicts. Bonus - if time: Integrating fastAPI with Sphinx. Demonstrate how to integrate OpenAPI with your Sphinx documentation. Based on experience building a data hub in asset management, the talk will show how fastAPI is built and how well introspection can help you understand what is going on under the hood and which library is actually doing the heavy lifting where.","title":"pydantic"},{"location":"08_fast_ape/#why-fast-api","text":"High performance web framework instead of flask custom python package interface to FASTAPI current version 259 classes 6022 lines of code more docstrings","title":"Why Fast API"},{"location":"09_pandas_polar/","text":"Pandas 2, Dask or Polars? Quickly tackling larger data on a single machine 06-03, 15:45\u201316:25 (Europe/London), Salisbury Pandas 2 brings new Arrow data types, faster calculations and better scalability. Dask scales Pandas across cores. Polars is a new competitor to Pandas designed around Arrow with native multicore support. Which should you choose for modern research workflows? We'll solve a \"just about fits in ram\" data task using the 3 solutions, talking about the pros and cons so you can make the best choice for your research workflow. You'll leave with a clear idea of whether Pandas 2, Dask or Polars is the tool for your team to invest in. Do you still need 5x working RAM for Pandas operations (probably not!)? Can Pandas string operations actually be fast (sure)? Since Polars uses Arrow data structures, can we easily use tools like Scikit-learn and matplotlib (yes-maybe)? What limits do we still face? How well does Dask handle the updates from Pandas 2? We'll discuss all of these questions along with timed results so you'll have evidence to take back to your team. Giles Weaver Data scientist. Domain expertise in maritime shipping (AIS). User of PySpark & Dask for over five years. Formerly a bioinformatician. Available for contract work. Ian Ozsvald Ian is a Chief Data Scientist, has helped co-organise the annual PyDataLondon conference raising $100k+ annually for the open source movement along with the associated 11,000+ member monthly meetup. Using data science he's helped clients find $2M in recoverable fraud, created the core IP which opened funding rounds for automated recruitment start-ups and diagnosed how major media companies can better supply recommendations to viewers. He gives conference talks internationally often as keynote speaker and is the author of the bestselling O'Reilly book High Performance Python (2nd edition). He has over 25 years of experience as a senior data science leader, trainer and team coach. For fun he's walked by his high-energy Springer Spaniel, surfs the Cornish coast and drinks fine coffee. Past talks and articles can be found at: Pandas is 15 years old Pandas 2 has PyArrow first class storage Internal clean-ups so less RAM used Can use pandas with SQL Polar is Rust based - python front-end, 3 years old dfple.lazy() Nan in pandas is missing Polars can have missing and Nan [High performance python]!(https://www.oreilly.com/library/view/high-performance-python/9781492055013/)","title":"Pandas 2"},{"location":"09_pandas_polar/#pandas-2-dask-or-polars","text":"Quickly tackling larger data on a single machine 06-03, 15:45\u201316:25 (Europe/London), Salisbury Pandas 2 brings new Arrow data types, faster calculations and better scalability. Dask scales Pandas across cores. Polars is a new competitor to Pandas designed around Arrow with native multicore support. Which should you choose for modern research workflows? We'll solve a \"just about fits in ram\" data task using the 3 solutions, talking about the pros and cons so you can make the best choice for your research workflow. You'll leave with a clear idea of whether Pandas 2, Dask or Polars is the tool for your team to invest in. Do you still need 5x working RAM for Pandas operations (probably not!)? Can Pandas string operations actually be fast (sure)? Since Polars uses Arrow data structures, can we easily use tools like Scikit-learn and matplotlib (yes-maybe)? What limits do we still face? How well does Dask handle the updates from Pandas 2? We'll discuss all of these questions along with timed results so you'll have evidence to take back to your team. Giles Weaver Data scientist. Domain expertise in maritime shipping (AIS). User of PySpark & Dask for over five years. Formerly a bioinformatician. Available for contract work. Ian Ozsvald Ian is a Chief Data Scientist, has helped co-organise the annual PyDataLondon conference raising $100k+ annually for the open source movement along with the associated 11,000+ member monthly meetup. Using data science he's helped clients find $2M in recoverable fraud, created the core IP which opened funding rounds for automated recruitment start-ups and diagnosed how major media companies can better supply recommendations to viewers. He gives conference talks internationally often as keynote speaker and is the author of the bestselling O'Reilly book High Performance Python (2nd edition). He has over 25 years of experience as a senior data science leader, trainer and team coach. For fun he's walked by his high-energy Springer Spaniel, surfs the Cornish coast and drinks fine coffee. Past talks and articles can be found at: Pandas is 15 years old Pandas 2 has PyArrow first class storage Internal clean-ups so less RAM used Can use pandas with SQL Polar is Rust based - python front-end, 3 years old dfple.lazy() Nan in pandas is missing Polars can have missing and Nan [High performance python]!(https://www.oreilly.com/library/view/high-performance-python/9781492055013/)","title":"Pandas 2, Dask or Polars?"},{"location":"10_new_dev_pandas_and_dask/","text":"New Developments in Pandas and Dask Dataframes 06-04, 10:15\u201310:55 (Europe/London), Salisbury We're in a new era of dataframe development. Libraries like Arrow, Polars, DuckDB, Vaex, Modin, and others stretch the bounds of performance on what we think can be done with tabular data in Python. These systems have great benchmarking results and generate significant buzz on social media. Pandas, the community favorite, is also innovating, although with less buzz. Structural improvements like Arrow data types, copy on write, and more bring the world's most popular dataframe library (55% of Python users) into significantly better performance and memory use. Additionally Dask, a parallel computing library developed closely with Pandas, has also added new features in the last year, like memory-stable shuffling, task queueing, and with recent experiments in query optimization which we'll discuss as well. In this talk we'll highlight some of these new features and show the impact they make on speed and cost on real-world workloads, as well as a vision for future development. We're in a new era of dataframe development. Libraries like Arrow, Polars, DuckDB, Vaex, Modin, and others stretch the bounds of performance on what we think can be done with tabular data in Python. These systems have great benchmarking results and generate significant buzz on social media. Pandas, the community favorite, is also innovating, although with less buzz. Structural improvements like Arrow data types, copy on write, and more bring the world's most popular dataframe library (55% of Python users) into significantly better performance and memory use. Additionally Dask, a parallel computing library developed closely with Pandas, has also added new features in the last year, like memory-stable shuffling, task queueing, and with recent experiments in query optimization which we'll discuss as well. In this talk we'll highlight some of these new features and show the impact they make on speed and cost on real-world workloads, as well as a vision for future development. Matthew Rocklin Matthew is an open source software developer in the numeric Python ecosystem. He maintains several PyData libraries, but today focuses mostly on Dask a library for scalable computing. Matthew worked for Anaconda Inc for several years, then built out the Dask team at NVIDIA for RAPIDS, and most recently founded Coiled to improve Python's scalability with Dask for large organizations. Matthew holds a bachelors degree from UC Berkeley in physics and mathematics, and a PhD in computer science from the University of Chicago. Website: https://matthewrocklin.com Dask: https://dask.org/ Coiled: https://coiled.io","title":"New Developments in Pandas and Dask Dataframes"},{"location":"10_new_dev_pandas_and_dask/#new-developments-in-pandas-and-dask-dataframes","text":"06-04, 10:15\u201310:55 (Europe/London), Salisbury We're in a new era of dataframe development. Libraries like Arrow, Polars, DuckDB, Vaex, Modin, and others stretch the bounds of performance on what we think can be done with tabular data in Python. These systems have great benchmarking results and generate significant buzz on social media. Pandas, the community favorite, is also innovating, although with less buzz. Structural improvements like Arrow data types, copy on write, and more bring the world's most popular dataframe library (55% of Python users) into significantly better performance and memory use. Additionally Dask, a parallel computing library developed closely with Pandas, has also added new features in the last year, like memory-stable shuffling, task queueing, and with recent experiments in query optimization which we'll discuss as well. In this talk we'll highlight some of these new features and show the impact they make on speed and cost on real-world workloads, as well as a vision for future development. We're in a new era of dataframe development. Libraries like Arrow, Polars, DuckDB, Vaex, Modin, and others stretch the bounds of performance on what we think can be done with tabular data in Python. These systems have great benchmarking results and generate significant buzz on social media. Pandas, the community favorite, is also innovating, although with less buzz. Structural improvements like Arrow data types, copy on write, and more bring the world's most popular dataframe library (55% of Python users) into significantly better performance and memory use. Additionally Dask, a parallel computing library developed closely with Pandas, has also added new features in the last year, like memory-stable shuffling, task queueing, and with recent experiments in query optimization which we'll discuss as well. In this talk we'll highlight some of these new features and show the impact they make on speed and cost on real-world workloads, as well as a vision for future development. Matthew Rocklin Matthew is an open source software developer in the numeric Python ecosystem. He maintains several PyData libraries, but today focuses mostly on Dask a library for scalable computing. Matthew worked for Anaconda Inc for several years, then built out the Dask team at NVIDIA for RAPIDS, and most recently founded Coiled to improve Python's scalability with Dask for large organizations. Matthew holds a bachelors degree from UC Berkeley in physics and mathematics, and a PhD in computer science from the University of Chicago. Website: https://matthewrocklin.com Dask: https://dask.org/ Coiled: https://coiled.io","title":"New Developments in Pandas and Dask Dataframes"},{"location":"11_correlation_to_causation/","text":"From correlations to causality in machine learning\u2013 a gentle guide to causal inference 06-04, 11:00\u201311:40 (Europe/London), Salisbury Today most conventional ML systems look to exploit correlations in data in order to draw inferences. However as we learned back in school Statistics class, correlation is not causation. So when you need to know the \u2018why\u2019 behind a particular prediction, or why A outperforms B in an experiment, then relying on correlations is insufficient. Furthermore some ML models are build purely for explainability and insight purposes rather than predictions, in order to understand how the world works so we could potentially make some kind of policy change, e.g. What if we had chosen a different strategy or tactic \u2013 would the outcome have been different, and if so, by how much? To answer these kinds of questions, you need to delve into the world of causality. This talk is a gentle (and occasionally entertaining) introduction to the interdisciplinary field of causality and how it is starting to impact machine learning. You will learn what kinds of questions causal inference can answer, and how it can address some of the limitations of current explainable ML methods, under certain conditions. I draw upon use-cases drawn from financial services and marketing, and I will show a short practical example of how combining human domain knowledge (intuitively via Graphical Causal Models) along with your data can sometimes unlock insights not recoverable by purely data driven approaches. The increasing ubiquity of ML systems in daily life, sometimes in high-stakes decision making, means there is a lot more interest in trust and explainability in such systems. For example, the forthcoming regulatory proposals by the EU (the \u2018AI Act\u2019). Today most conventional ML systems look to exploit correlations in data in order to draw inferences. However as we learned back in school Statistics class, correlation is not causation (often side-stepping the question \u2018in that case, what is causation?\u2019). Often, if all one cares about are accurate, low-stakes predictions, then the distinction doesn\u2019t matter. However, when you need to know the \u2018why\u2019 behind a particular prediction, relying on correlations alone is insufficient. Furthermore, some ML models are build purely for explainability purposes, rather than making predictions, in order to gain insights into how a particular real-world system works, with the intent of making some policy change that will (hopefully) improve some outcome of interest. In such cases even state-of-the-industry explainability methods like SHAP don\u2019t work well. In my work I get asked a lot of questions along the lines of What If? What we had chosen a different strategy or tactic, would the outcome have been different, and if so by how much?? What levers can we pull going forward to improve business performance? To answer these kinds of questions, we need to delve into the world of causality. This talk is a light hearted introduction to the field of causality and how it intersects with the machine learning field. You will learn what kinds of questions causal inference can answer, and how it can address some of the limitations of current explainable ML methods, under certain conditions, with use-cases drawn from financial services and marketing. Fields like economics and medicine have been asking these questions for years, to attempt to provide robust answers to hard to measure, social policy questions (the 2021 Nobel Prize for Economics was shared between 2 research teams that used novel causal inference approaches in education and labour policy). But in recent times, there are been some cross pollination with Machine Learning, and indeed, some notable scholars are actively engaged in using causality to help design the next generation of machine learning problems (causal representational learning). So it\u2019s a contemporary and upwardly trending topic. This talk assumes no background in causality and only a rudimentary understanding of machine learning or statistics would be helpful. There is no maths or equations presented here. In summary, this talk is divided into 3 parts: 1. A broad introduction to causal inference and where I use it in my financial services work 2. The intersection of where causality meets machine learning 3. A practical example where a causal inference-based approach (through intuitive Graphical Causal Models, Python code provided) in concert with your data beats the standard correlation-based approaches in terms of being able to uncover the true casual effects. 4. Where to go for more information - I\u2019ll provide a curated list of resources for the interested reader. Steve Goodman has 20 years experience in data analytics and data science, mostly in the fields of marketing, consulting and financial services. He is currently a Data Science Lead at Tide, a financial services platform based in London. He holds a PhD in Applied Statistics and a MBA.","title":"From correlations to causality in machine learning\u2013 a gentle guide to causal inference"},{"location":"11_correlation_to_causation/#from-correlations-to-causality-in-machine-learning-a-gentle-guide-to-causal-inference","text":"06-04, 11:00\u201311:40 (Europe/London), Salisbury Today most conventional ML systems look to exploit correlations in data in order to draw inferences. However as we learned back in school Statistics class, correlation is not causation. So when you need to know the \u2018why\u2019 behind a particular prediction, or why A outperforms B in an experiment, then relying on correlations is insufficient. Furthermore some ML models are build purely for explainability and insight purposes rather than predictions, in order to understand how the world works so we could potentially make some kind of policy change, e.g. What if we had chosen a different strategy or tactic \u2013 would the outcome have been different, and if so, by how much? To answer these kinds of questions, you need to delve into the world of causality. This talk is a gentle (and occasionally entertaining) introduction to the interdisciplinary field of causality and how it is starting to impact machine learning. You will learn what kinds of questions causal inference can answer, and how it can address some of the limitations of current explainable ML methods, under certain conditions. I draw upon use-cases drawn from financial services and marketing, and I will show a short practical example of how combining human domain knowledge (intuitively via Graphical Causal Models) along with your data can sometimes unlock insights not recoverable by purely data driven approaches. The increasing ubiquity of ML systems in daily life, sometimes in high-stakes decision making, means there is a lot more interest in trust and explainability in such systems. For example, the forthcoming regulatory proposals by the EU (the \u2018AI Act\u2019). Today most conventional ML systems look to exploit correlations in data in order to draw inferences. However as we learned back in school Statistics class, correlation is not causation (often side-stepping the question \u2018in that case, what is causation?\u2019). Often, if all one cares about are accurate, low-stakes predictions, then the distinction doesn\u2019t matter. However, when you need to know the \u2018why\u2019 behind a particular prediction, relying on correlations alone is insufficient. Furthermore, some ML models are build purely for explainability purposes, rather than making predictions, in order to gain insights into how a particular real-world system works, with the intent of making some policy change that will (hopefully) improve some outcome of interest. In such cases even state-of-the-industry explainability methods like SHAP don\u2019t work well. In my work I get asked a lot of questions along the lines of What If? What we had chosen a different strategy or tactic, would the outcome have been different, and if so by how much?? What levers can we pull going forward to improve business performance? To answer these kinds of questions, we need to delve into the world of causality. This talk is a light hearted introduction to the field of causality and how it intersects with the machine learning field. You will learn what kinds of questions causal inference can answer, and how it can address some of the limitations of current explainable ML methods, under certain conditions, with use-cases drawn from financial services and marketing. Fields like economics and medicine have been asking these questions for years, to attempt to provide robust answers to hard to measure, social policy questions (the 2021 Nobel Prize for Economics was shared between 2 research teams that used novel causal inference approaches in education and labour policy). But in recent times, there are been some cross pollination with Machine Learning, and indeed, some notable scholars are actively engaged in using causality to help design the next generation of machine learning problems (causal representational learning). So it\u2019s a contemporary and upwardly trending topic. This talk assumes no background in causality and only a rudimentary understanding of machine learning or statistics would be helpful. There is no maths or equations presented here. In summary, this talk is divided into 3 parts: 1. A broad introduction to causal inference and where I use it in my financial services work 2. The intersection of where causality meets machine learning 3. A practical example where a causal inference-based approach (through intuitive Graphical Causal Models, Python code provided) in concert with your data beats the standard correlation-based approaches in terms of being able to uncover the true casual effects. 4. Where to go for more information - I\u2019ll provide a curated list of resources for the interested reader. Steve Goodman has 20 years experience in data analytics and data science, mostly in the fields of marketing, consulting and financial services. He is currently a Data Science Lead at Tide, a financial services platform based in London. He holds a PhD in Applied Statistics and a MBA.","title":"From correlations to causality in machine learning\u2013 a gentle guide to causal inference"}]}